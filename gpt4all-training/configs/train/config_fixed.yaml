# Fixed Configuration for GPT4All Training
# Based on user intent and train.py requirements

# Model and Tokenizer
model_name: "distilgpt2"
tokenizer_name: "distilgpt2"
gradient_checkpointing: true
save_name: "gpt4all-prod"

# Dataset
streaming: false
num_proc: 1
dataset_path: "../../data/train.jsonl"
max_length: 1024
batch_size: 1

# Training Dynamics
lr: 2.0e-5
min_lr: 0
weight_decay: 0.01
save_every: 1000
log_grads_every: 100
log_lr_every: 100
eval_every: 1000
output_dir: "../../output/gpt4all-prod/"
checkpoint: null
lora: false
warmup_steps: 500
num_epochs: 10
gradient_accumulation_steps: 4

# Logging
wandb: false
wandb_entity: ""
wandb_project_name: "gpt4all-prod"
seed: 42
push_to_hub: false
